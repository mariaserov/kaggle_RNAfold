{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "First, we need to decide what library can be used for handling data. I will encode sequences numerically, and want to have the option to handle them that way, scale to add different information (BPP, physical properties, distance, etc), and to handle them as graphs. The best one for this seems to be PyTorch (datatype - tensor), it also has the option of PyTorch Geometric. Another option would be TensorFlow/Keras, though it seems a bit harder to handle graphs. \n",
    "\n",
    "TO DO:\n",
    "- set up first NN with X as input and y (coordinates) as output\n",
    "- incorporate MSA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data (X & y)\n",
    "For now, these are prepared as tensors of one-hot-encoded sequence (padded to make sure they are of same length), and tensors of coordinates. MSA are not yet considered.\n",
    "Update: since embedding is used, the sequences are instead converted to tensors. The one-hot-encode code is kept below for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mx/tkk24h4127zdqbx5pfzb0_7m0000gn/T/ipykernel_53330/1647122054.py:12: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  train_lbl = train_lbl.interpolate() # For now, interpolate - are there better imputation techniques?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>resname</th>\n",
       "      <th>resid</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>ID_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, resname, resid, x_1, y_1, z_1, ID_num]\n",
       "Index: []"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_seq = pd.read_csv(\"../toy_data/train_sequences.csv\")\n",
    "train_lbl = pd.read_csv(\"../toy_data/train_labels.csv\")\n",
    "train_lbl = train_lbl.interpolate() # For now, interpolate - are there better imputation techniques?\n",
    "\n",
    "train_lbl[\"ID_num\"] = [n+1 for n in range(len(train_lbl))] # map ID to numeric ID to store in tensor\n",
    "id_mapping = {idx+1: og_id for idx, og_id in enumerate(train_lbl['ID'])} # create mapping to re-map back to original ID\n",
    "id_mapping[0] = \"padded_row\"\n",
    "\n",
    "train_lbl[train_lbl.iloc[:,3:6].isna().any(axis=1)] # Check which rows  rows have NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order corresponds between sequences and coordinates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataset & Dataloader\n",
    "\n",
    "nts = ['G', 'U', 'C', 'A', 'X', '-']\n",
    "mapping = {nt: idx+1 for idx, nt in enumerate(nts)}\n",
    "reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "\n",
    "\n",
    "def tokenise_seq(seq, mapping=mapping):\n",
    "    seq_idx = [mapping[nt] for nt in seq]\n",
    "    seq_idx = torch.tensor(seq_idx)\n",
    "    return seq_idx\n",
    "\n",
    "def make_coord_tensor(train_lbl):\n",
    "    train_lbl['base_ID'] = train_lbl['ID'].str.rsplit('_', n=1).str[0] # sequence ID for each nt\n",
    "    main_id_list = train_lbl['ID']\n",
    "    y_list = []\n",
    "    og_id_list_temp = [] # not extended list\n",
    "    for idx in list(train_lbl['base_ID'].unique()):\n",
    "        subset = train_lbl[train_lbl['base_ID'] == idx]\n",
    "        coords = []\n",
    "        for res in range(len(subset['ID'])):\n",
    "            coord = list(subset.iloc[res, 3:6])\n",
    "            coords.append(coord)\n",
    "        \n",
    "        og_id_list_temp.append(torch.tensor(list(subset['ID_num'])))\n",
    "        \n",
    "        y_list.append(torch.tensor(coords, dtype=torch.float32))\n",
    "        \n",
    "    y_tensor = pad_sequence(y_list, batch_first=True)\n",
    "    og_id_list = pad_sequence(og_id_list_temp, batch_first=True)\n",
    "\n",
    "    return y_list, y_tensor, og_id_list\n",
    "\n",
    "class Rnadataset(Dataset):\n",
    "    def __init__(self, train_seq, train_lbl):\n",
    "        super().__init__()\n",
    "        self.X_list = [tokenise_seq(seq) for seq in train_seq['sequence']]\n",
    "        self.X_tensor = pad_sequence(self.X_list, batch_first=True)\n",
    "        \n",
    "        self.y_list, self.y_tensor, self.ids = make_coord_tensor(train_lbl)\n",
    "        if all(train_lbl[\"base_ID\"].unique() == train_seq['target_id']): # Always good to check\n",
    "            print(\"Order corresponds between sequences and coordinates\")\n",
    "        else:\n",
    "            raise ValueError(\"Mismatch between base_IDs in train_lbl and target_ids in train_seq.\")\n",
    "            \n",
    "        #self.ids = train_seq['target_id']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_tensor)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        return self.X_tensor[index], self.y_tensor[index], self.ids[index]\n",
    "    \n",
    "dataset = Rnadataset(train_seq, train_lbl)\n",
    "\n",
    "train_size = int(len(dataset)*0.8)\n",
    "test_size = int(len(dataset)-train_size)\n",
    "\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=15, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=15, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on loss function\n",
    "\n",
    "The competition uses TM-Score to evaluate predictions, which among other things is based on distance rather than absolute differences. As such, for my task, I will be converting both ground truth and predicted coordinates to distance matrices, and minimising loss between the two. Since it leverages  squared difference in distances, we'll use MSE (for now)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build initial simple model\n",
    "The architecture will consist of:\n",
    "- embedding: mapping integers corresponding to nucleotides in sequence to vectors representing semantic meanings\n",
    "- sequence encoder:  inspired by RibonanzaNet: 9 layers of 1D conv + residual, multi-head self-attention, and a feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define blocks of the model\n",
    "\n",
    "class SeqEncoder(nn.Module): # Define single encoder block\n",
    "    def __init__(self, hidden_size=256, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding = kernel_size // 2)\n",
    "        self.attn = nn.MultiheadAttention(hidden_size, 8)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.norm3 = nn.LayerNorm(hidden_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4*hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, padding_mask=None):\n",
    "        X = X + self.conv(X.transpose(1,2)).transpose(1,2) # 1D conv with residual connection + Layer Norm; transpose to expected input\n",
    "        X = self.norm1(X)\n",
    "        res = X\n",
    "        attn_out, _ = self.attn(X.transpose(0,1), X.transpose(0,1), X.transpose(0,1), key_padding_mask=padding_mask)\n",
    "        attn_out = attn_out.transpose(0,1) + res\n",
    "        X = self.norm2(attn_out)\n",
    "        res = X\n",
    "        X = self.norm3(res + self.ff(X))\n",
    "        return X\n",
    "        \n",
    "class ConvEncoder(nn.Module): # define a whole transformer pipeline\n",
    "    def __init__(self, n_blocks = 9, **kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([SeqEncoder(**kwargs) for _ in range(n_blocks)])\n",
    "    \n",
    "    def forward(self, X, padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X, padding_mask=padding_mask)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model \n",
    "\n",
    "class InitModel(Module): # define rest of model\n",
    "    def __init__(self, vocab=6, max_len = 1024, n_blocks=9, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.b = vocab\n",
    "        self.embedding = nn.Embedding(self.b, hidden_size, padding_idx=0) # map each base to a vector representation of size 256\n",
    "        self.pos_embedding = nn.Embedding( max_len, hidden_size)\n",
    "        self.convencoder = ConvEncoder(n_blocks=n_blocks, hidden_size=hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Make embeddings (+ positional embeddings)\n",
    "\n",
    "        pad_mask = (X == 0)\n",
    "        seq_length = X.size()[1]\n",
    "\n",
    "        X = self.embedding(X)\n",
    "        positions = torch.arange(seq_length).unsqueeze(0).expand(X.size(0), seq_length)\n",
    "        pos_embd = self.pos_embedding(positions)\n",
    "        X = X + pos_embd\n",
    "\n",
    "        # Pass through convolutional transformer\n",
    "\n",
    "        X = self.convencoder(X, padding_mask=pad_mask)\n",
    "\n",
    "        out = self.output(X)\n",
    "        return(out)\n",
    "\n",
    "        ## TO DO: add padding masks, add layers which map the encoded representations to coords, add distance calculation, minimise loss btwn og dist & encoded dist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss function on distance matrices rather than coords\n",
    "\n",
    "def pairwise_distance_matrix(X):\n",
    "    diff = X.unsqueeze(2) - X.unsqueeze(1)  # shape: (batch, 35, 35, 5)\n",
    "    return torch.norm(diff, dim=-1)\n",
    "\n",
    "class DistanceMatrixLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = MSELoss()\n",
    "    \n",
    "    def forward(self, y_true, y_pred, padding_mask):\n",
    "        y_true_m = pairwise_distance_matrix(y_true)\n",
    "        y_pred_m = pairwise_distance_matrix(y_pred)\n",
    "\n",
    "        valid = (~padding_mask).unsqueeze(2) & (~padding_mask).unsqueeze(1)\n",
    "        se = (y_true_m - y_pred_m).pow(2)\n",
    "        se_valid = se[valid]\n",
    "        return se_valid.mean()\n",
    "\n",
    "        # loss = self.loss(y_true_m, y_pred_m)\n",
    "        # return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss train 325.49, Loss Test 146.37\n",
      "Epoch 2: Loss train 185.34, Loss Test 133.18\n",
      "Epoch 3: Loss train 151.37, Loss Test 120.84\n",
      "Epoch 4: Loss train 134.64, Loss Test 123.84\n",
      "Epoch 5: Loss train 123.61, Loss Test 116.11\n",
      "Epoch 6: Loss train 117.24, Loss Test 112.5\n",
      "Epoch 7: Loss train 107.13, Loss Test 109.23\n",
      "Epoch 8: Loss train 99.75, Loss Test 109.34\n",
      "Epoch 9: Loss train 93.66, Loss Test 115.37\n",
      "Epoch 10: Loss train 89.26, Loss Test 113.94\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "# Define function to convert coordinates to dataframe for TMScore calculation\n",
    "def coords_to_df_train(tensor_list):\n",
    "    flat_tensor = torch.cat(tensor_list, dim=0).flatten(0,1) # fuse tensors in list, then flatten (batch + seq)\n",
    "\n",
    "    n_seq = 0\n",
    "    seq_length = tensor_list[0].size()[1]\n",
    "    for i in tensor_list: # calculate number of sequences \n",
    "        n_seq = n_seq + i.size()[0] \n",
    "    \n",
    "    seq_ids = torch.arange(n_seq).repeat_interleave(seq_length).unsqueeze(1) # create ID for each seq in flat tensor\n",
    "    pred_idxs = torch.cat([seq_ids, flat_tensor], dim=1) # fuse IDs with tensor itself\n",
    "    df = pd.DataFrame(pred_idxs.detach().numpy()) # convert to dataframe\n",
    "    df.columns = ['seq_ID_int', \"x\", \"y\", \"z\"] \n",
    "    return df \n",
    "\n",
    "\n",
    "initmodel = InitModel()\n",
    "criterion = DistanceMatrixLoss()\n",
    "optimiser = Adam(initmodel.parameters())\n",
    "\n",
    "cols = [\"Epoch\", \"Train_Loss\", \"Test_Loss\"]\n",
    "perf = pd.DataFrame(index=range(n_epochs), columns=cols)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train = []\n",
    "    epoch_pred_train = []\n",
    "    epoch_true_train = []\n",
    "    num_ids = []\n",
    "    seq_idx = []\n",
    "    initmodel.train()\n",
    "    for seq, coords, ids in train_loader:\n",
    "        pad_mask = (seq == 0)\n",
    "        optimiser.zero_grad()\n",
    "        pred_coords = initmodel(seq)\n",
    "        loss = criterion(coords,pred_coords, pad_mask)\n",
    "        loss_train.append(loss.item())\n",
    "        epoch_pred_train.append(pred_coords.detach())\n",
    "        epoch_true_train.append(coords.detach())\n",
    "        num_ids.extend(ids.flatten(0,1).tolist())\n",
    "        seq_idx.extend(seq.flatten(0,1).tolist())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    loss_train = sum(loss_train)/len(loss_train)\n",
    "\n",
    "    initmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_test = []\n",
    "        epoch_pred_test = []\n",
    "        epoch_true_test = []\n",
    "        for seq, coords, ids in test_loader:\n",
    "            pad_mask = (seq == 0)\n",
    "            pred_coords_test = initmodel(seq)\n",
    "            loss = criterion(coords, pred_coords_test, pad_mask)\n",
    "            epoch_true_test.extend(coords)\n",
    "            epoch_pred_test.extend(pred_coords_test)\n",
    "            loss_test.append(loss.item())\n",
    "        \n",
    "        loss_test_val = sum(loss_test)/len(loss_test)\n",
    "    \n",
    "    perf.iloc[epoch, :] = [epoch+1, loss_train, loss_test]\n",
    "    print(f\"Epoch {epoch+1}: Loss train {round(loss_train, 2)}, Loss Test {round(loss_test_val, 2)}\")\n",
    "\n",
    "\n",
    "# TO DO: figure out TM Score (expects 2D dataframe of values), add padding masks, refine whole model, and train on full data, figure out how to import from src/func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf.to_csv(\"../outputs/InitialModel/initial_perf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order corresponds between sequences and coordinates\n"
     ]
    }
   ],
   "source": [
    "# Get validation set\n",
    "\n",
    "validation_seq = pd.read_csv(\"../data/validation_sequences.csv\")\n",
    "validation_lbl = pd.read_csv(\"../data/validation_labels.csv\")\n",
    "validation_lbl[\"ID_num\"] = [n+1 for n in range(len(validation_lbl))] # map ID to numeric ID to store in tensor\n",
    "id_mapping_val = {idx+1: og_id for idx, og_id in enumerate(validation_lbl['ID'])} # create mapping to re-map back to original ID\n",
    "id_mapping_val[0] = \"padded_row\"\n",
    "val_set = Rnadataset(validation_seq, validation_lbl)\n",
    "\n",
    "# Make predictions on validation set\n",
    "initmodel.eval()\n",
    "val_pred = initmodel(val_set.X_tensor)\n",
    "\n",
    "mask_val = (val_set.X_tensor.flatten(0,1) != 0)\n",
    "val_pred_flat = val_pred.flatten(0,1)\n",
    "val_seq_pred = val_pred_flat[mask_val]\n",
    "\n",
    "submission_cols = ['ID', 'resname', 'resid', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2', 'z_2',\n",
    "       'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "submission_df = pd.DataFrame(0.0, index = range(val_seq_pred.shape[0]), columns = submission_cols)\n",
    "submission_df[['ID', 'resname', 'resid']] = validation_lbl[['ID', 'resname', 'resid']]\n",
    "submission_df[['x_1', 'y_1', 'z_1']] = val_seq_pred.detach().numpy()\n",
    "submission_df.dtypes\n",
    "submission_df.to_csv('../outputs/InitialModel/submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnafold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
