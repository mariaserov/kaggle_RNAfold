{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "First, we need to decide what library can be used for handling data. I will encode sequences numerically, and want to have the option to handle them that way, scale to add different information (BPP, physical properties, distance, etc), and to handle them as graphs. The best one for this seems to be PyTorch (datatype - tensor), it also has the option of PyTorch Geometric. Another option would be TensorFlow/Keras, though it seems a bit harder to handle graphs. \n",
    "\n",
    "TO DO:\n",
    "- set up first NN with X as input and y (coordinates) as output\n",
    "- incorporate MSA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data (X & y)\n",
    "For now, these are prepared as tensors of one-hot-encoded sequence (padded to make sure they are of same length), and tensors of coordinates. MSA are not yet considered.\n",
    "Update: since embedding is used, the sequences are instead converted to tensors. The one-hot-encode code is kept below for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_seq = pd.read_csv(\"../toy_data/train_sequences.csv\")\n",
    "train_lbl = pd.read_csv(\"../toy_data/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(train_lbl[\"base_ID\"].unique() == train_seq['target_id']) # Always good to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 35, 3])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_coord_tensor(train_lbl):\n",
    "    train_lbl['base_ID'] = train_lbl['ID'].str.rsplit('_', n=1).str[0] # sequence ID for each nt\n",
    "    main_id_list = train_lbl['ID']\n",
    "    y_list = []\n",
    "    og_id_list_temp = [] # not extended list\n",
    "    for idx in list(train_lbl['base_ID'].unique()):\n",
    "        subset = train_lbl[train_lbl['base_ID'] == idx]\n",
    "        coords = []\n",
    "        for res in range(len(subset['ID'])):\n",
    "            coord = list(subset.iloc[res, 3:6])\n",
    "            coords.append(coord)\n",
    "        \n",
    "        og_id_list_temp.append(list(subset['ID']))\n",
    "        \n",
    "        y_list.append(torch.tensor(coords, dtype=torch.float32))\n",
    "        \n",
    "    y_tensor = pad_sequence(y_list, batch_first=True)\n",
    "\n",
    "    og_id_list = []\n",
    "    for list_ids in og_id_list_temp:\n",
    "        extension = ['0'] * ( int(y_tensor.size()[1]) - len(list_ids) )\n",
    "        list_ids.extend(extension)\n",
    "        og_id_list.append(list_ids)\n",
    "\n",
    "    return y_list, y_tensor, og_id_list\n",
    "\n",
    "temp1, temp2, temp3 = make_coord_tensor(train_lbl)\n",
    "temp2.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataset & Dataloader\n",
    "\n",
    "nts = ['G', 'U', 'C', 'A', 'X', '-']\n",
    "mapping = {nt: idx+1 for idx, nt in enumerate(nts)}\n",
    "\n",
    "\n",
    "def tokenise_seq(seq, mapping=mapping):\n",
    "    seq_idx = [mapping[nt] for nt in seq]\n",
    "    seq_idx = torch.tensor(seq_idx)\n",
    "    return seq_idx\n",
    "\n",
    "def make_coord_tensor(train_lbl):\n",
    "    train_lbl['base_ID'] = train_lbl['ID'].str.rsplit('_', n=1).str[0] # sequence ID for each nt\n",
    "    main_id_list = train_lbl['ID']\n",
    "    y_list = []\n",
    "    og_id_list_temp = [] # not extended list\n",
    "    for idx in list(train_lbl['base_ID'].unique()):\n",
    "        subset = train_lbl[train_lbl['base_ID'] == idx]\n",
    "        coords = []\n",
    "        for res in range(len(subset['ID'])):\n",
    "            coord = list(subset.iloc[res, 3:6])\n",
    "            coords.append(coord)\n",
    "        \n",
    "        og_id_list_temp.append(list(subset['ID']))\n",
    "        \n",
    "        y_list.append(torch.tensor(coords, dtype=torch.float32))\n",
    "        \n",
    "    y_tensor = pad_sequence(y_list, batch_first=True)\n",
    "\n",
    "    og_id_list = []\n",
    "    for list_ids in og_id_list_temp:\n",
    "        extension = ['0'] * ( int(y_tensor.size()[1]) - len(list_ids) )\n",
    "        list_ids.extend(extension)\n",
    "        og_id_list.append(list_ids)\n",
    "\n",
    "    return y_list, y_tensor, og_id_list\n",
    "\n",
    "class Rnadataset(Dataset):\n",
    "    def __init__(self, train_seq, train_lbl):\n",
    "        super().__init__()\n",
    "        self.X_list = [tokenise_seq(seq) for seq in train_seq['sequence']]\n",
    "        self.X_tensor = pad_sequence(self.X_list, batch_first=True)\n",
    "        self.y_list, self.y_tensor, self.ids = make_coord_tensor(train_lbl)\n",
    "        #self.ids = train_seq['target_id']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_tensor)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        return self.X_tensor[index], self.y_tensor[index], self.ids[index]\n",
    "    \n",
    "\n",
    "dataset = Rnadataset(train_seq, train_lbl)\n",
    "\n",
    "train_size = int(len(dataset)*0.8)\n",
    "test_size = int(len(dataset)-train_size)\n",
    "\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=15, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=15, shuffle=False)\n",
    "\n",
    "len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1ATW_A_1',\n",
       "  '1RNG_A_1',\n",
       "  '1MME_D_1',\n",
       "  '1ZIG_A_1',\n",
       "  '1TLR_A_1',\n",
       "  '1SLO_A_1',\n",
       "  '1AFX_A_1',\n",
       "  '1KPD_A_1',\n",
       "  '1SCL_A_1',\n",
       "  '1HMH_E_1',\n",
       "  '1IKD_A_1',\n",
       "  '1AQO_A_1',\n",
       "  '1RHT_A_1',\n",
       "  '1ATO_A_1',\n",
       "  '1KAJ_A_1'),\n",
       " ('1ATW_A_2',\n",
       "  '1RNG_A_2',\n",
       "  '1MME_D_2',\n",
       "  '1ZIG_A_2',\n",
       "  '1TLR_A_2',\n",
       "  '1SLO_A_2',\n",
       "  '1AFX_A_2',\n",
       "  '1KPD_A_2',\n",
       "  '1SCL_A_2',\n",
       "  '1HMH_E_2',\n",
       "  '1IKD_A_2',\n",
       "  '1AQO_A_2',\n",
       "  '1RHT_A_2',\n",
       "  '1ATO_A_2',\n",
       "  '1KAJ_A_2'),\n",
       " ('1ATW_A_3',\n",
       "  '1RNG_A_3',\n",
       "  '1MME_D_3',\n",
       "  '1ZIG_A_3',\n",
       "  '1TLR_A_3',\n",
       "  '1SLO_A_3',\n",
       "  '1AFX_A_3',\n",
       "  '1KPD_A_3',\n",
       "  '1SCL_A_3',\n",
       "  '1HMH_E_3',\n",
       "  '1IKD_A_3',\n",
       "  '1AQO_A_3',\n",
       "  '1RHT_A_3',\n",
       "  '1ATO_A_3',\n",
       "  '1KAJ_A_3'),\n",
       " ('1ATW_A_4',\n",
       "  '1RNG_A_4',\n",
       "  '1MME_D_4',\n",
       "  '1ZIG_A_4',\n",
       "  '1TLR_A_4',\n",
       "  '1SLO_A_4',\n",
       "  '1AFX_A_4',\n",
       "  '1KPD_A_4',\n",
       "  '1SCL_A_4',\n",
       "  '1HMH_E_4',\n",
       "  '1IKD_A_4',\n",
       "  '1AQO_A_4',\n",
       "  '1RHT_A_4',\n",
       "  '1ATO_A_4',\n",
       "  '1KAJ_A_4'),\n",
       " ('1ATW_A_5',\n",
       "  '1RNG_A_5',\n",
       "  '1MME_D_5',\n",
       "  '1ZIG_A_5',\n",
       "  '1TLR_A_5',\n",
       "  '1SLO_A_5',\n",
       "  '1AFX_A_5',\n",
       "  '1KPD_A_5',\n",
       "  '1SCL_A_5',\n",
       "  '1HMH_E_5',\n",
       "  '1IKD_A_5',\n",
       "  '1AQO_A_5',\n",
       "  '1RHT_A_5',\n",
       "  '1ATO_A_5',\n",
       "  '1KAJ_A_5'),\n",
       " ('1ATW_A_6',\n",
       "  '1RNG_A_6',\n",
       "  '1MME_D_6',\n",
       "  '1ZIG_A_6',\n",
       "  '1TLR_A_6',\n",
       "  '1SLO_A_6',\n",
       "  '1AFX_A_6',\n",
       "  '1KPD_A_6',\n",
       "  '1SCL_A_6',\n",
       "  '1HMH_E_6',\n",
       "  '1IKD_A_6',\n",
       "  '1AQO_A_6',\n",
       "  '1RHT_A_6',\n",
       "  '1ATO_A_6',\n",
       "  '1KAJ_A_6'),\n",
       " ('1ATW_A_7',\n",
       "  '1RNG_A_7',\n",
       "  '1MME_D_7',\n",
       "  '1ZIG_A_7',\n",
       "  '1TLR_A_7',\n",
       "  '1SLO_A_7',\n",
       "  '1AFX_A_7',\n",
       "  '1KPD_A_7',\n",
       "  '1SCL_A_7',\n",
       "  '1HMH_E_7',\n",
       "  '1IKD_A_7',\n",
       "  '1AQO_A_7',\n",
       "  '1RHT_A_7',\n",
       "  '1ATO_A_7',\n",
       "  '1KAJ_A_7'),\n",
       " ('1ATW_A_8',\n",
       "  '1RNG_A_8',\n",
       "  '1MME_D_8',\n",
       "  '1ZIG_A_8',\n",
       "  '1TLR_A_8',\n",
       "  '1SLO_A_8',\n",
       "  '1AFX_A_8',\n",
       "  '1KPD_A_8',\n",
       "  '1SCL_A_8',\n",
       "  '1HMH_E_8',\n",
       "  '1IKD_A_8',\n",
       "  '1AQO_A_8',\n",
       "  '1RHT_A_8',\n",
       "  '1ATO_A_8',\n",
       "  '1KAJ_A_8'),\n",
       " ('1ATW_A_9',\n",
       "  '1RNG_A_9',\n",
       "  '1MME_D_9',\n",
       "  '1ZIG_A_9',\n",
       "  '1TLR_A_9',\n",
       "  '1SLO_A_9',\n",
       "  '1AFX_A_9',\n",
       "  '1KPD_A_9',\n",
       "  '1SCL_A_9',\n",
       "  '1HMH_E_9',\n",
       "  '1IKD_A_9',\n",
       "  '1AQO_A_9',\n",
       "  '1RHT_A_9',\n",
       "  '1ATO_A_9',\n",
       "  '1KAJ_A_9'),\n",
       " ('1ATW_A_10',\n",
       "  '1RNG_A_10',\n",
       "  '1MME_D_10',\n",
       "  '1ZIG_A_10',\n",
       "  '1TLR_A_10',\n",
       "  '1SLO_A_10',\n",
       "  '1AFX_A_10',\n",
       "  '1KPD_A_10',\n",
       "  '1SCL_A_10',\n",
       "  '1HMH_E_10',\n",
       "  '1IKD_A_10',\n",
       "  '1AQO_A_10',\n",
       "  '1RHT_A_10',\n",
       "  '1ATO_A_10',\n",
       "  '1KAJ_A_10'),\n",
       " ('1ATW_A_11',\n",
       "  '1RNG_A_11',\n",
       "  '1MME_D_11',\n",
       "  '1ZIG_A_11',\n",
       "  '1TLR_A_11',\n",
       "  '1SLO_A_11',\n",
       "  '1AFX_A_11',\n",
       "  '1KPD_A_11',\n",
       "  '1SCL_A_11',\n",
       "  '1HMH_E_11',\n",
       "  '1IKD_A_11',\n",
       "  '1AQO_A_11',\n",
       "  '1RHT_A_11',\n",
       "  '1ATO_A_11',\n",
       "  '1KAJ_A_11'),\n",
       " ('1ATW_A_12',\n",
       "  '1RNG_A_12',\n",
       "  '1MME_D_12',\n",
       "  '1ZIG_A_12',\n",
       "  '1TLR_A_12',\n",
       "  '1SLO_A_12',\n",
       "  '1AFX_A_12',\n",
       "  '1KPD_A_12',\n",
       "  '1SCL_A_12',\n",
       "  '1HMH_E_12',\n",
       "  '1IKD_A_12',\n",
       "  '1AQO_A_12',\n",
       "  '1RHT_A_12',\n",
       "  '1ATO_A_12',\n",
       "  '1KAJ_A_12'),\n",
       " ('1ATW_A_13',\n",
       "  '0',\n",
       "  '1MME_D_13',\n",
       "  '0',\n",
       "  '1TLR_A_13',\n",
       "  '1SLO_A_13',\n",
       "  '0',\n",
       "  '1KPD_A_13',\n",
       "  '1SCL_A_13',\n",
       "  '1HMH_E_13',\n",
       "  '1IKD_A_13',\n",
       "  '1AQO_A_13',\n",
       "  '1RHT_A_13',\n",
       "  '1ATO_A_13',\n",
       "  '1KAJ_A_13'),\n",
       " ('1ATW_A_14',\n",
       "  '0',\n",
       "  '1MME_D_14',\n",
       "  '0',\n",
       "  '1TLR_A_14',\n",
       "  '1SLO_A_14',\n",
       "  '0',\n",
       "  '1KPD_A_14',\n",
       "  '1SCL_A_14',\n",
       "  '1HMH_E_14',\n",
       "  '1IKD_A_14',\n",
       "  '1AQO_A_14',\n",
       "  '1RHT_A_14',\n",
       "  '1ATO_A_14',\n",
       "  '1KAJ_A_14'),\n",
       " ('1ATW_A_15',\n",
       "  '0',\n",
       "  '1MME_D_15',\n",
       "  '0',\n",
       "  '1TLR_A_15',\n",
       "  '1SLO_A_15',\n",
       "  '0',\n",
       "  '1KPD_A_15',\n",
       "  '1SCL_A_15',\n",
       "  '1HMH_E_15',\n",
       "  '1IKD_A_15',\n",
       "  '1AQO_A_15',\n",
       "  '1RHT_A_15',\n",
       "  '1ATO_A_15',\n",
       "  '1KAJ_A_15'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_16',\n",
       "  '0',\n",
       "  '1TLR_A_16',\n",
       "  '1SLO_A_16',\n",
       "  '0',\n",
       "  '1KPD_A_16',\n",
       "  '1SCL_A_16',\n",
       "  '1HMH_E_16',\n",
       "  '1IKD_A_16',\n",
       "  '1AQO_A_16',\n",
       "  '1RHT_A_16',\n",
       "  '1ATO_A_16',\n",
       "  '1KAJ_A_16'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_17',\n",
       "  '0',\n",
       "  '1TLR_A_17',\n",
       "  '1SLO_A_17',\n",
       "  '0',\n",
       "  '1KPD_A_17',\n",
       "  '1SCL_A_17',\n",
       "  '1HMH_E_17',\n",
       "  '1IKD_A_17',\n",
       "  '1AQO_A_17',\n",
       "  '1RHT_A_17',\n",
       "  '1ATO_A_17',\n",
       "  '1KAJ_A_17'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_18',\n",
       "  '0',\n",
       "  '1TLR_A_18',\n",
       "  '1SLO_A_18',\n",
       "  '0',\n",
       "  '1KPD_A_18',\n",
       "  '1SCL_A_18',\n",
       "  '1HMH_E_18',\n",
       "  '1IKD_A_18',\n",
       "  '1AQO_A_18',\n",
       "  '1RHT_A_18',\n",
       "  '1ATO_A_18',\n",
       "  '1KAJ_A_18'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_19',\n",
       "  '0',\n",
       "  '1TLR_A_19',\n",
       "  '1SLO_A_19',\n",
       "  '0',\n",
       "  '1KPD_A_19',\n",
       "  '1SCL_A_19',\n",
       "  '1HMH_E_19',\n",
       "  '1IKD_A_19',\n",
       "  '1AQO_A_19',\n",
       "  '1RHT_A_19',\n",
       "  '1ATO_A_19',\n",
       "  '1KAJ_A_19'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_20',\n",
       "  '0',\n",
       "  '1TLR_A_20',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_20',\n",
       "  '1SCL_A_20',\n",
       "  '1HMH_E_20',\n",
       "  '1IKD_A_20',\n",
       "  '1AQO_A_20',\n",
       "  '1RHT_A_20',\n",
       "  '0',\n",
       "  '1KAJ_A_20'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_21',\n",
       "  '0',\n",
       "  '1TLR_A_21',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_21',\n",
       "  '1SCL_A_21',\n",
       "  '1HMH_E_21',\n",
       "  '1IKD_A_21',\n",
       "  '1AQO_A_21',\n",
       "  '1RHT_A_21',\n",
       "  '0',\n",
       "  '1KAJ_A_21'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_22',\n",
       "  '0',\n",
       "  '1TLR_A_22',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_22',\n",
       "  '1SCL_A_22',\n",
       "  '1HMH_E_22',\n",
       "  '1IKD_A_22',\n",
       "  '1AQO_A_22',\n",
       "  '1RHT_A_22',\n",
       "  '0',\n",
       "  '1KAJ_A_22'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_23',\n",
       "  '0',\n",
       "  '1TLR_A_23',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_23',\n",
       "  '1SCL_A_23',\n",
       "  '1HMH_E_23',\n",
       "  '0',\n",
       "  '1AQO_A_23',\n",
       "  '1RHT_A_23',\n",
       "  '0',\n",
       "  '1KAJ_A_23'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_24',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_24',\n",
       "  '1SCL_A_24',\n",
       "  '1HMH_E_24',\n",
       "  '0',\n",
       "  '1AQO_A_24',\n",
       "  '1RHT_A_24',\n",
       "  '0',\n",
       "  '1KAJ_A_24'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '1MME_D_25',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_25',\n",
       "  '1SCL_A_25',\n",
       "  '1HMH_E_25',\n",
       "  '0',\n",
       "  '1AQO_A_25',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KAJ_A_25'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_26',\n",
       "  '1SCL_A_26',\n",
       "  '1HMH_E_26',\n",
       "  '0',\n",
       "  '1AQO_A_26',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KAJ_A_26'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_27',\n",
       "  '1SCL_A_27',\n",
       "  '1HMH_E_27',\n",
       "  '0',\n",
       "  '1AQO_A_27',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KAJ_A_27'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_28',\n",
       "  '1SCL_A_28',\n",
       "  '1HMH_E_28',\n",
       "  '0',\n",
       "  '1AQO_A_28',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KAJ_A_28'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_29',\n",
       "  '1SCL_A_29',\n",
       "  '1HMH_E_29',\n",
       "  '0',\n",
       "  '1AQO_A_29',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KAJ_A_29'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_30',\n",
       "  '0',\n",
       "  '1HMH_E_30',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KAJ_A_30'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_31',\n",
       "  '0',\n",
       "  '1HMH_E_31',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KAJ_A_31'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KPD_A_32',\n",
       "  '0',\n",
       "  '1HMH_E_32',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1KAJ_A_32'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1HMH_E_33',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0'),\n",
       " ('0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1HMH_E_34',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0'),\n",
       " ('0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0')]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list_temp = next(iter(train_loader))\n",
    "\n",
    "list_temp[1].size()\n",
    "list_temp[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nts = ['G', 'U', 'C', 'A', 'X', '-']\n",
    "# mapping = {nt: idx+1 for idx, nt in enumerate(nts)}\n",
    "\n",
    "\n",
    "# def tokenise_seq(seq, mapping=mapping):\n",
    "#     seq_idx = [mapping[nt] for nt in seq]\n",
    "#     seq_idx = torch.tensor(seq_idx)\n",
    "#     return seq_idx\n",
    "\n",
    "# X_list = [tokenise_seq(seq) for seq in train_seq['sequence']]\n",
    "# X_tensor = pad_sequence(X_list, batch_first=True)\n",
    "\n",
    "# X_tensor[0] # QC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y: Convert coordinates to tensor\n",
    "\n",
    "# train_lbl['base_ID'] = train_lbl['ID'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# y_list = []\n",
    "# for idx in list(train_lbl['base_ID'].unique()):\n",
    "\n",
    "#     coords = []\n",
    "#     for res in range(len(train_lbl[train_lbl['ID'].str.startswith(idx)])):\n",
    "#         coord = list(train_lbl.iloc[res, 3:6])\n",
    "#         coords.append(coord)\n",
    "    \n",
    "#     y_list.append(torch.tensor(coords, dtype=torch.float32))\n",
    "    \n",
    "# y_tensor = pad_sequence(y_list, batch_first=True)\n",
    "\n",
    "# y_tensor.size()[0:2] == X_tensor.size()[0:2] # check that it's formatted correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! Make a padding mask\n",
    "\n",
    "attn_mask = []\n",
    "for seq in X_list:\n",
    "    mask = [False if i < len(seq) else True for i in range(X_tensor.size()[1])]\n",
    "    attn_mask.append(mask)\n",
    "\n",
    "attn_mask = torch.tensor(attn_mask)\n",
    "padding_mask = ~attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Dataset & Dataloader\n",
    "\n",
    "# from torch.utils.data import random_split\n",
    "\n",
    "# class Rnadataset(Dataset):\n",
    "#     def __init__(self, X_tensor, y_tensor):\n",
    "#         super().__init__()\n",
    "#         self.X_tensor = X_tensor\n",
    "#         self.y_tensor = y_tensor\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.X_tensor)\n",
    "    \n",
    "#     def __getitem__(self, index) :\n",
    "#         return self.X_tensor[index], self.y_tensor[index]\n",
    "    \n",
    "# dataset = Rnadataset(X_tensor, y_tensor)\n",
    "\n",
    "# train_size = int(len(dataset)*0.8)\n",
    "# test_size = int(len(dataset)-train_size)\n",
    "\n",
    "# train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on loss function\n",
    "\n",
    "The competition uses TM-Score to evaluate predictions, which among other things is based on distance rather than absolute differences. As such, for my task, I will be converting both ground truth and predicted coordinates to distance matrices, and minimising loss between the two. Since it leverages  squared difference in distances, we'll use MSE (for now)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build initial simple model\n",
    "The architecture will consist of:\n",
    "- embedding: mapping integers corresponding to nucleotides in sequence to vectors representing semantic meanings\n",
    "- sequence encoder:  inspired by RibonanzaNet: 9 layers of 1D conv + residual, multi-head self-attention, and a feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define blocks of the model\n",
    "\n",
    "class SeqEncoder(nn.Module): # Define single encoder block\n",
    "    def __init__(self, hidden_size=256, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding = kernel_size // 2)\n",
    "        self.attn = nn.MultiheadAttention(hidden_size, 8)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.norm3 = nn.LayerNorm(hidden_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4*hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.conv(X.transpose(1,2)).transpose(1,2) # 1D conv with residual connection + Layer Norm; transpose to expected input\n",
    "        X = self.norm1(X)\n",
    "        res = X\n",
    "        attn_out, _ = self.attn(X.transpose(0,1), X.transpose(0,1), X.transpose(0,1))\n",
    "        attn_out = attn_out.transpose(0,1) + res\n",
    "        X = self.norm2(attn_out)\n",
    "        res = X\n",
    "        X = self.norm3(res + self.ff(X))\n",
    "        return X\n",
    "        \n",
    "class ConvEncoder(nn.Module): # define a whole transformer pipeline\n",
    "    def __init__(self, n_blocks = 9, **kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([SeqEncoder(**kwargs) for _ in range(n_blocks)])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model \n",
    "\n",
    "class InitModel(Module): # define rest of model\n",
    "    def __init__(self, seq_length=35, vocab=6, n_blocks=9, hidden_size=256):\n",
    "        super().__init__()\n",
    "        #self.b, self.l = X.size()\n",
    "        self.l = seq_length\n",
    "        self.b = vocab\n",
    "        self.embedding = nn.Embedding(self.l , hidden_size, padding_idx=0) # map each base to a vector representation of size 256\n",
    "        self.pos_embedding = nn.Embedding(self.l , hidden_size)\n",
    "        self.convencoder = ConvEncoder(n_blocks=n_blocks, hidden_size=hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Make embeddings (+ positional embeddings)\n",
    "\n",
    "        X = self.embedding(X)\n",
    "        positions = torch.arange(self.l).unsqueeze(0).expand(X.size(0), self.l)\n",
    "        pos_embd = self.pos_embedding(positions)\n",
    "        X = X + pos_embd\n",
    "\n",
    "        # Pass through convolutional transformer\n",
    "\n",
    "        X = self.convencoder(X)\n",
    "\n",
    "        out = self.output(X)\n",
    "        return(out)\n",
    "\n",
    "        ## TO DO: add padding masks, add layers which map the encoded representations to coords, add distance calculation, minimise loss btwn og dist & encoded dist \n",
    "\n",
    "\n",
    "initmodel = InitModel()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss function on distance matrices rather than coords\n",
    "\n",
    "def pairwise_distance_matrix(X):\n",
    "    diff = X.unsqueeze(2) - X.unsqueeze(1)  # shape: (batch, 35, 35, 5)\n",
    "    return torch.norm(diff, dim=-1)\n",
    "\n",
    "class DistanceMatrixLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = MSELoss()\n",
    "    \n",
    "    def forward(self, y_true, y_pred):\n",
    "        y_true_m = pairwise_distance_matrix(y_true)\n",
    "        y_pred_m = pairwise_distance_matrix(y_pred)\n",
    "        loss = self.loss(y_true_m, y_pred_m)\n",
    "        return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func import score\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "# Define function to convert coordinates to dataframe for TMScore calculation\n",
    "def coords_to_df_train(tensor_list):\n",
    "    flat_tensor = torch.cat(tensor_list, dim=0).flatten(0,1) # fuse tensors in list, then flatten (batch + seq)\n",
    "\n",
    "    n_seq = 0\n",
    "    seq_length = tensor_list[0].size()[1]\n",
    "    for i in tensor_list: # calculate number of sequences \n",
    "        n_seq = n_seq + i.size()[0] \n",
    "    \n",
    "    seq_ids = torch.arange(n_seq).repeat_interleave(seq_length).unsqueeze(1) # create ID for each seq in flat tensor\n",
    "    pred_idxs = torch.cat([seq_ids, flat_tensor], dim=1) # fuse IDs with tensor itself\n",
    "    df = pd.DataFrame(pred_idxs.detach().numpy()) # convert to dataframe\n",
    "    df.columns = ['seq_ID_int', \"x_1\", \"y_1\", \"z_1\"] \n",
    "    df['seq_ID_int'] = df['seq_ID_int'].astype(int) # change to integer\n",
    "    return df \n",
    "\n",
    "\n",
    "initmodel = InitModel()\n",
    "criterion = DistanceMatrixLoss()\n",
    "optimiser = Adam(initmodel.parameters())\n",
    "\n",
    "cols = [\"Epoch\", \"Train_Loss\", \"Test_Loss\", \"Train_TMScore\", \"Test_TMScore\"]\n",
    "perf = pd.DataFrame(index=range(n_epochs), columns=cols)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train = []\n",
    "    epoch_pred_train = []\n",
    "    epoch_true_train = []\n",
    "    target_ids = []\n",
    "    seq_idx = []\n",
    "    initmodel.train()\n",
    "    for seq, coords, ids in train_loader:\n",
    "        optimiser.zero_grad()\n",
    "        pred_coords = initmodel(seq)\n",
    "        loss = criterion(coords,pred_coords)\n",
    "        loss_train.append(loss.item())\n",
    "        epoch_pred_train.append(pred_coords.detach())\n",
    "        epoch_true_train.append(coords.detach())\n",
    "        target_ids.extend(list(ids))\n",
    "        seq_idx.extend(seq.flatten(0,1).tolist())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    \n",
    "    \n",
    "    df_pred_train = coords_to_df_train(epoch_pred_train)\n",
    "    df_true_train = coords_to_df_train(epoch_true_train)\n",
    "\n",
    "    len(df_pred_train)\n",
    "\n",
    "    df_pred_train['seq_idx'] = seq_idx\n",
    "    df_true_train['seq_idx'] = seq_idx\n",
    "\n",
    "    df_pred_train = df_pred_train[df_pred_train['seq_idx'] != 0]\n",
    "\n",
    "    #df_pred_train['ID'] = target_ids\n",
    "    #df_true_train['ID'] = target_ids\n",
    "    \n",
    "    # ids_mapping = {seq_n: seq_id for seq_n, seq_id in enumerate(target_ids)} # map integer IDs to actual seq/target IDs\n",
    "    # df_pred_train['target_id'] = df_pred_train['ID'].map(ids_mapping)\n",
    "    # df_true_train['target_id'] = df_true_train['ID'].map(ids_mapping)\n",
    "    \n",
    "    tm_train = score(df_pred_train, df_true_train, row_id_column_name = 'temp_seq_id')\n",
    "    loss_train = sum(loss_train)/len(loss_train)\n",
    "\n",
    "    initmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_test = []\n",
    "        epoch_pred_test = []\n",
    "        epoch_true_test = []\n",
    "        for seq, coords in test_loader:\n",
    "            pred_coords_test = initmodel(seq)\n",
    "            epoch_true_test.extend(coords)\n",
    "            epoch_pred_test.extend(pred_coords_test)\n",
    "            loss_test.append(criterion(coords, pred_coords_test).item())\n",
    "        \n",
    "        #TMTest = score(pd.DataFrame(epoch_true_test), pd.DataFrame(epoch_pred_test))\n",
    "        loss_test = sum(loss_test)/len(loss_test)\n",
    "    \n",
    "    #perf.iloc[epoch, :] = [epoch+1, loss_train, loss_test, TMTrain, TMTest]\n",
    "    print(f\"Epoch {epoch+1}: Loss train {round(loss_train, 2)}, Loss Test {round(loss_test, 2)}, TM TRAIN {tm_train}\")\n",
    "\n",
    "\n",
    "# TO DO: figure out TM Score (expects 2D dataframe of values), add padding masks, refine whole model, and train on full data, figure out how to import from src/func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_pred_train['seq_idx'])\n",
    "\n",
    "#len(train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.ids)\n",
    "\n",
    "#df_pred_train['ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([280, 5])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred_coords.flatten(0,1).size()\n",
    " \n",
    "# FOR ONE SEQUENCE\n",
    "\n",
    "\n",
    "seq_ids = torch.arange(batch_size).repeat_interleave(35).unsqueeze(1)\n",
    "epoch_no = torch.Tensor([10]).repeat(35*8).unsqueeze(1)\n",
    "\n",
    "pred_flat = pred_coords.flatten(0, 1)\n",
    "seq_ids.size()\n",
    "pred_idxs = torch.cat([epoch_no, seq_ids, pred_flat], dim=1)\n",
    "pred_idxs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(torch.cat(epoch_pred_train, dim=0).flatten(0, 1).detach().numpy())\n",
    "df.columns = [\"x\", \"y\", \"z\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_seq_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.760</td>\n",
       "      <td>-25.974001</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9.310</td>\n",
       "      <td>-29.638000</td>\n",
       "      <td>2.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5.529</td>\n",
       "      <td>-27.813000</td>\n",
       "      <td>5.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2.678</td>\n",
       "      <td>-24.900999</td>\n",
       "      <td>9.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.827</td>\n",
       "      <td>-20.136000</td>\n",
       "      <td>11.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     temp_seq_id       x          y       z\n",
       "0              0  13.760 -25.974001   0.102\n",
       "1              0   9.310 -29.638000   2.669\n",
       "2              0   5.529 -27.813000   5.878\n",
       "3              0   2.678 -24.900999   9.793\n",
       "4              0   1.827 -20.136000  11.793\n",
       "..           ...     ...        ...     ...\n",
       "835           23   0.000   0.000000   0.000\n",
       "836           23   0.000   0.000000   0.000\n",
       "837           23   0.000   0.000000   0.000\n",
       "838           23   0.000   0.000000   0.000\n",
       "839           23   0.000   0.000000   0.000\n",
       "\n",
       "[840 rows x 4 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO THIS FOR epoch_pred_train & epoch_true_train - smth different for test\n",
    "def coords_to_df_train(tensor):\n",
    "    flat_tensor = torch.cat(tensor, dim=0).flatten(0,1) # fuse tensors in list, then flatten (batch + seq)\n",
    "\n",
    "    n_seq = 0\n",
    "    seq_length = tensor[0].size()[1]\n",
    "    for i in tensor: # calculate number of sequences \n",
    "        n_seq = n_seq + i.size()[0] \n",
    "    \n",
    "    seq_ids = torch.arange(n_seq).repeat_interleave(seq_length).unsqueeze(1) # create ID for each seq in flat tensor\n",
    "    pred_idxs = torch.cat([seq_ids, flat_tensor], dim=1) # fuse IDs with tensor itself\n",
    "    df = pd.DataFrame(pred_idxs.detach().numpy()) # convert to dataframe\n",
    "    df.columns = [\"temp_seq_id\", \"x\", \"y\", \"z\"] \n",
    "    df['temp_seq_id'] = df['temp_seq_id'].astype(int) # change to integer\n",
    "    return df \n",
    "\n",
    "flat_tensor = epoch_true_train\n",
    "\n",
    "df_testing = coords_to_df_train(flat_tensor)\n",
    "df_testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([840, 3])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_list = epoch_pred_train\n",
    "flat_tensor.size()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnafold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
