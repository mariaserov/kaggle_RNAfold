{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting distances\n",
    "\n",
    "The model 1 learnt to predict the actual 3D coordinates of each nucleotide. However, this means that it was also learning the rotation in space, which for the purpose of this task is irrelevant - we want rotation & translation invariance. In this next iteration, the model will predict distances, which will be deterministically mapped back to coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data (X & y)\n",
    "For now, these are prepared as tensors of one-hot-encoded sequence (padded to make sure they are of same length), and tensors of coordinates. MSA are not yet considered.\n",
    "Update: since embedding is used, the sequences are instead converted to tensors. The one-hot-encode code is kept below for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.1156215.pbs/ipykernel_3224507/2255570638.py:12: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  train_lbl = train_lbl.interpolate() # For now, interpolate - are there better imputation techniques?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_seq = pd.read_csv(\"../toy_data/train_sequences.csv\")\n",
    "train_lbl = pd.read_csv(\"../toy_data/train_labels.csv\")\n",
    "train_lbl = train_lbl.interpolate() # For now, interpolate - are there better imputation techniques?\n",
    "\n",
    "train_lbl[\"ID_num\"] = [n+1 for n in range(len(train_lbl))] # map ID to numeric ID to store in tensor\n",
    "id_mapping = {idx+1: og_id for idx, og_id in enumerate(train_lbl['ID'])} # create mapping to re-map back to original ID\n",
    "id_mapping[0] = \"padded_row\"\n",
    "\n",
    "#train_lbl[train_lbl.iloc[:,3:6].isna().any(axis=1)] # Check which rows  rows have NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order corresponds between sequences and coordinates\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset & Dataloader\n",
    "\n",
    "nts = ['G', 'U', 'C', 'A', 'X', '-']\n",
    "mapping = {nt: idx+1 for idx, nt in enumerate(nts)}\n",
    "reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "\n",
    "\n",
    "def tokenise_seq(seq, mapping=mapping):\n",
    "    seq_idx = [mapping[nt] for nt in seq]\n",
    "    seq_idx = torch.tensor(seq_idx)\n",
    "    return seq_idx\n",
    "\n",
    "def make_coord_tensor(train_lbl):\n",
    "    train_lbl['base_ID'] = train_lbl['ID'].str.rsplit('_', n=1).str[0] # sequence ID for each nt\n",
    "    main_id_list = train_lbl['ID']\n",
    "    y_list = []\n",
    "    og_id_list_temp = [] # not extended list\n",
    "    for idx in list(train_lbl['base_ID'].unique()):\n",
    "        subset = train_lbl[train_lbl['base_ID'] == idx]\n",
    "        coords = []\n",
    "        for res in range(len(subset['ID'])):\n",
    "            coord = list(subset.iloc[res, 3:6])\n",
    "            coords.append(coord)\n",
    "        \n",
    "        og_id_list_temp.append(torch.tensor(list(subset['ID_num'])))\n",
    "        \n",
    "        y_list.append(torch.tensor(coords, dtype=torch.float32))\n",
    "        \n",
    "    y_tensor = pad_sequence(y_list, batch_first=True)\n",
    "    og_id_list = pad_sequence(og_id_list_temp, batch_first=True)\n",
    "\n",
    "    return y_list, y_tensor, og_id_list\n",
    "\n",
    "# Create Dataset & Dataloader\n",
    "\n",
    "def collate(batch):\n",
    "    xs, ys, ids = zip(*batch)\n",
    "    len_x = [x.size(0) for x in xs]\n",
    "\n",
    "    x_padded = pad_sequence(xs, batch_first=True)\n",
    "    y_padded = pad_sequence(ys, batch_first=True)\n",
    "    id_padded = pad_sequence(ids, batch_first=True)\n",
    "\n",
    "    return x_padded, y_padded, id_padded, torch.tensor(len_x)\n",
    "\n",
    "\n",
    "nts = ['G', 'U', 'C', 'A', 'X', '-']\n",
    "mapping = {nt: idx+1 for idx, nt in enumerate(nts)}\n",
    "reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "\n",
    "\n",
    "def tokenise_seq(seq, mapping=mapping):\n",
    "    seq_idx = [mapping[nt] for nt in seq]\n",
    "    seq_idx = torch.tensor(seq_idx)\n",
    "    return seq_idx\n",
    "\n",
    "def make_coord_tensor(train_lbl):\n",
    "    train_lbl['base_ID'] = train_lbl['ID'].str.rsplit('_', n=1).str[0] # sequence ID for each nt\n",
    "    main_id_list = train_lbl['ID']\n",
    "    y_list = []\n",
    "    og_id_list_temp = [] # not extended list\n",
    "    for idx in list(train_lbl['base_ID'].unique()):\n",
    "        subset = train_lbl[train_lbl['base_ID'] == idx]\n",
    "        coords = []\n",
    "        for res in range(len(subset['ID'])):\n",
    "            coord = list(subset.iloc[res, 3:6])\n",
    "            coords.append(coord)\n",
    "        \n",
    "        og_id_list_temp.append(torch.tensor(list(subset['ID_num'])))\n",
    "        \n",
    "        y_list.append(torch.tensor(coords, dtype=torch.float32))\n",
    "        \n",
    "    #y_tensor = pad_sequence(y_list, batch_first=True)\n",
    "    og_id_list = pad_sequence(og_id_list_temp, batch_first=True)\n",
    "\n",
    "    return y_list, og_id_list\n",
    "\n",
    "class Rnadataset(Dataset):\n",
    "    def __init__(self, train_seq, train_lbl):\n",
    "        super().__init__()\n",
    "        self.X_list = [tokenise_seq(seq) for seq in train_seq['sequence']]\n",
    "        #self.X_tensor = pad_sequence(self.X_list, batch_first=True)\n",
    "        \n",
    "        self.y_list, self.ids = make_coord_tensor(train_lbl)\n",
    "        if all(train_lbl[\"base_ID\"].unique() == train_seq['target_id']): # Always good to check\n",
    "            print(\"Order corresponds between sequences and coordinates\")\n",
    "        else:\n",
    "            raise ValueError(\"Mismatch between base_IDs in train_lbl and target_ids in train_seq.\")\n",
    "            \n",
    "        #self.ids = train_seq['target_id']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_list)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        return self.X_list[index], self.y_list[index], self.ids[index]\n",
    "    \n",
    "dataset = Rnadataset(train_seq, train_lbl)\n",
    "\n",
    "train_size = int(len(dataset)*0.8)\n",
    "test_size = int(len(dataset)-train_size)\n",
    "\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=False, collate_fn=collate, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate, num_workers=8, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to map distances to coords & vica versa\n",
    "\n",
    "Using classical multidimensional scaling cMDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_to_coords(D):\n",
    "    L = D.shape[0] # D: (L, L) symmetric, zero diagonal\n",
    "    I = torch.eye(L) # Centering matrix - LxL identity\n",
    "    ones = torch.ones((L, L)) / L\n",
    "    H = I - ones\n",
    "\n",
    "    D2 = D**2 # Squared distances\n",
    "    B = -0.5 * H @ D2 @ H # Double‐centered Gram matrix\n",
    "\n",
    "    # Eigen‐decomposition\n",
    "    eigvals, eigvecs = torch.linalg.eigh(B)\n",
    "    # Sort descending\n",
    "    idx = torch.argsort(eigvals, descending=True)\n",
    "    vals = eigvals[idx][:3]\n",
    "    vecs = eigvecs[:, idx][:, :3]\n",
    "\n",
    "    # Coordinates = V * sqrt(Λ)\n",
    "    return vecs * torch.sqrt(vals).unsqueeze(0)\n",
    "\n",
    "# Define custom loss function on distance matrices rather than coords\n",
    "\n",
    "def pairwise_distance_matrix(X):\n",
    "    diff = X.unsqueeze(2) - X.unsqueeze(1)  # shape: (batch, 35, 35, 5)\n",
    "    return torch.norm(diff, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build initial simple model\n",
    "The main architecture will be the same as in the initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define blocks of the model\n",
    "\n",
    "class SeqEncoder(nn.Module): # Define single encoder block\n",
    "    def __init__(self, hidden_size=256, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding = kernel_size // 2)\n",
    "        self.attn = nn.MultiheadAttention(hidden_size, 8)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.norm3 = nn.LayerNorm(hidden_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4*hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, padding_mask=None):\n",
    "        X = X + self.conv(X.transpose(1,2)).transpose(1,2) # 1D conv with residual connection + Layer Norm; transpose to expected input\n",
    "        X = self.norm1(X)\n",
    "        res = X\n",
    "        attn_out, _ = self.attn(X.transpose(0,1), X.transpose(0,1), X.transpose(0,1), key_padding_mask=padding_mask)\n",
    "        attn_out = attn_out.transpose(0,1) + res\n",
    "        X = self.norm2(attn_out)\n",
    "        res = X\n",
    "        X = self.norm3(res + self.ff(X))\n",
    "        return X\n",
    "        \n",
    "class ConvEncoder(nn.Module): # define a whole transformer pipeline\n",
    "    def __init__(self, n_blocks = 9, **kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([SeqEncoder(**kwargs) for _ in range(n_blocks)])\n",
    "    \n",
    "    def forward(self, X, padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X, padding_mask=padding_mask)\n",
    "        return X\n",
    "    \n",
    "class DistancePredictor(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        f = 4 * hidden_size\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(f, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, 1))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        b, l, d = X.size()\n",
    "        Xi = X.unsqueeze(2).expand(-1, -1, l, -1) # position i\n",
    "        Xj = X.unsqueeze(1).expand(-1, l, -1, -1) # position j\n",
    "        f = torch.cat([Xi, Xj, Xi-Xj, Xi*Xj], dim = -1) # stack i & j repr, their distance (-), and similarity (*)\n",
    "        d = self.mlp(f).squeeze(-1)\n",
    "        d = torch.relu(d)\n",
    "        d = (d+d.transpose(1,2))*0.5 # symmetric\n",
    "        d = d.masked_fill(torch.eye(l).bool(), 0.) # 0 across diagonal\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model \n",
    "\n",
    "class InitModel(Module): # define rest of model\n",
    "    def __init__(self, vocab=6, max_len = 1024, n_blocks=9, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.b = vocab\n",
    "        self.embedding = nn.Embedding(self.b, hidden_size, padding_idx=0) # map each base to a vector representation of size 256\n",
    "        self.pos_embedding = nn.Embedding( max_len, hidden_size)\n",
    "        self.convencoder = ConvEncoder(n_blocks=n_blocks, hidden_size=hidden_size)\n",
    "        self.output=DistancePredictor(hidden_size=hidden_size)\n",
    "        #self.output = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Make embeddings (+ positional embeddings)\n",
    "\n",
    "        pad_mask = (X == 0)\n",
    "        seq_length = X.size()[1]\n",
    "\n",
    "        X = self.embedding(X)\n",
    "        positions = torch.arange(seq_length).unsqueeze(0).expand(X.size(0), seq_length)\n",
    "        pos_embd = self.pos_embedding(positions)\n",
    "        X = X + pos_embd\n",
    "\n",
    "        # Pass through convolutional transformer\n",
    "\n",
    "        X = self.convencoder(X, padding_mask=pad_mask)\n",
    "\n",
    "        out = self.output(X)\n",
    "        return(out)\n",
    "\n",
    "        ## TO DO: add padding masks, add layers which map the encoded representations to coords, add distance calculation, minimise loss btwn og dist & encoded dist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief interlude to test compilers for training speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm        44.53%     189.373ms        44.54%     189.444ms     681.454us           278  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.32%       1.355ms        37.28%     158.548ms       2.734ms            58  \n",
      "                                         AddmmBackward0         0.17%     715.001us        36.50%     155.216ms       2.676ms            58  \n",
      "                                           aten::linear         0.15%     654.011us        22.56%      95.960ms     856.785us           112  \n",
      "                                            aten::addmm        17.06%      72.560ms        18.02%      76.641ms       1.321ms            58  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.05%     195.675us         7.30%      31.042ms       1.725ms            18  \n",
      "                                   ConvolutionBackward0         0.04%     158.954us         7.25%      30.847ms       1.714ms            18  \n",
      "                             aten::convolution_backward         5.93%      25.206ms         7.22%      30.688ms       1.705ms            18  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.08%     343.728us         6.21%      26.430ms     489.440us            54  \n",
      "                                            MmBackward0         0.13%     560.541us         6.13%      26.086ms     483.075us            54  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 425.292ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No compiler\n",
    "\n",
    "initmodel = InitModel()\n",
    "#initmodel = torch.compile(initmodel, backend=\"aot_eager\")\n",
    "print(\"begin training\")\n",
    "with torch.autograd.profiler.profile(record_shapes=True) as prof:\n",
    "    initmodel.train()\n",
    "    loss_train = []\n",
    "    for i, (seq, coords, ids) in enumerate(train_loader):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        pad_mask = (seq == 0)\n",
    "        optimiser.zero_grad()\n",
    "        true = pairwise_distance_matrix(coords)\n",
    "        pred_i = initmodel(seq)\n",
    "        mask = (seq!=0).unsqueeze(1).expand_as(pred_i)\n",
    "        pred = pred_i[mask]\n",
    "        true = true[mask]\n",
    "        loss = criterion(pred,true)\n",
    "        loss_train.append(loss.item())\n",
    "        num_ids.extend(ids.flatten(0,1).tolist())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "# Dump the profiling table\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "autograd::engine::evaluate_function: CompiledFunctio...         0.06%     268.758us        63.29%     304.842ms     152.421ms             2  \n",
      "                               CompiledFunctionBackward         9.87%      47.532ms        63.24%     304.573ms     152.286ms             2  \n",
      "                                               aten::mm        37.84%     182.258ms        37.86%     182.340ms     655.898us           278  \n",
      "                             Torch-Compiled Region: 0/3         0.17%     795.004us        34.81%     167.649ms      83.824ms             2  \n",
      "                                       CompiledFunction         6.96%      33.512ms        34.64%     166.854ms      83.427ms             2  \n",
      "                                            aten::addmm        14.40%      69.349ms        15.04%      72.423ms       1.249ms            58  \n",
      "                             aten::convolution_backward         5.22%      25.163ms         6.33%      30.492ms       1.694ms            18  \n",
      "                                            aten::copy_         3.69%      17.761ms         3.69%      17.761ms      53.821us           330  \n",
      "                                              aten::add         3.56%      17.132ms         3.56%      17.132ms      80.055us           214  \n",
      "                                            aten::clone         0.16%     775.150us         3.39%      16.321ms      77.720us           210  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 481.640ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compiler\n",
    "\n",
    "initmodel = InitModel()\n",
    "initmodel = torch.compile(initmodel, backend=\"aot_eager\")\n",
    "print(\"begin training\")\n",
    "with torch.autograd.profiler.profile(record_shapes=True) as prof:\n",
    "    initmodel.train()\n",
    "    loss_train = []\n",
    "    for i, (seq, coords, ids) in enumerate(train_loader):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        pad_mask = (seq == 0)\n",
    "        optimiser.zero_grad()\n",
    "        true = pairwise_distance_matrix(coords)\n",
    "        pred_i = initmodel(seq)\n",
    "        mask = (seq!=0).unsqueeze(1).expand_as(pred_i)\n",
    "        pred = pred_i[mask]\n",
    "        true = true[mask]\n",
    "        loss = criterion(pred,true)\n",
    "        loss_train.append(loss.item())\n",
    "        num_ids.extend(ids.flatten(0,1).tolist())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "# Dump the profiling table\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm        44.79%     198.175ms        44.81%     198.251ms     713.134us           278  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.34%       1.515ms        37.48%     165.807ms       2.859ms            58  \n",
      "                                         AddmmBackward0         0.14%     634.489us        36.70%     162.370ms       2.799ms            58  \n",
      "                                                forward         0.39%       1.723ms        33.76%     149.362ms      74.681ms             2  \n",
      "                                           aten::linear         0.14%     634.164us        22.65%     100.223ms     894.846us           112  \n",
      "                                            aten::addmm        16.86%      74.595ms        17.79%      78.720ms       1.357ms            58  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.04%     159.728us         7.09%      31.364ms       1.742ms            18  \n",
      "                                   ConvolutionBackward1         0.03%     137.891us         7.05%      31.204ms       1.734ms            18  \n",
      "                             aten::convolution_backward         5.89%      26.039ms         7.02%      31.066ms       1.726ms            18  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.07%     318.633us         6.01%      26.574ms     492.108us            54  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 442.409ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trace\n",
    "\n",
    "initmodel = InitModel()\n",
    "example_seq, example_coords, example_ids = next(iter(train_loader))\n",
    "\n",
    "initmodel.eval()\n",
    "initmodel = torch.jit.trace(initmodel, example_seq)\n",
    "initmodel.train()\n",
    "print(\"begin training\")\n",
    "with torch.autograd.profiler.profile(record_shapes=True) as prof:\n",
    "    initmodel.train()\n",
    "    loss_train = []\n",
    "    for i, (seq, coords, ids) in enumerate(train_loader):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        pad_mask = (seq == 0)\n",
    "        optimiser.zero_grad()\n",
    "        true = pairwise_distance_matrix(coords)\n",
    "        pred_i = initmodel(seq)\n",
    "        mask = (seq!=0).unsqueeze(1).expand_as(pred_i)\n",
    "        pred = pred_i[mask]\n",
    "        true = true[mask]\n",
    "        loss = criterion(pred,true)\n",
    "        loss_train.append(loss.item())\n",
    "        num_ids.extend(ids.flatten(0,1).tolist())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "# Dump the profiling table\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss train 547.58, Loss Test 547.58\n",
      "Epoch 2: Loss train 447.32, Loss Test 447.32\n",
      "Epoch 3: Loss train 338.92, Loss Test 338.92\n",
      "Epoch 4: Loss train 270.27, Loss Test 270.27\n",
      "Epoch 5: Loss train 223.45, Loss Test 223.45\n",
      "Epoch 6: Loss train 192.55, Loss Test 192.55\n",
      "Epoch 7: Loss train 178.04, Loss Test 178.04\n",
      "Epoch 8: Loss train 178.37, Loss Test 178.37\n",
      "Epoch 9: Loss train 187.18, Loss Test 187.18\n",
      "Epoch 10: Loss train 192.23, Loss Test 192.23\n",
      "Time: 16.685160160064697s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "initmodel = InitModel()\n",
    "criterion = MSELoss()\n",
    "optimiser = Adam(initmodel.parameters())\n",
    "\n",
    "cols = [\"Epoch\", \"Train_Loss\", \"Test_Loss\"]\n",
    "perf = pd.DataFrame(index=range(n_epochs), columns=cols)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train = []\n",
    "    epoch_pred_train = []\n",
    "    epoch_true_train = []\n",
    "    num_ids = []\n",
    "    seq_idx = []\n",
    "    initmodel.train()\n",
    "    for seq, coords, ids, _ in train_loader:\n",
    "        pad_mask = (seq == 0)\n",
    "        optimiser.zero_grad()\n",
    "        true = pairwise_distance_matrix(coords)\n",
    "        pred_i = initmodel(seq)\n",
    "        mask = (seq!=0).unsqueeze(1).expand_as(pred_i)\n",
    "        pred = pred_i[mask]\n",
    "        true = true[mask]\n",
    "        loss = criterion(pred,true)\n",
    "        loss_train.append(loss.item())\n",
    "        num_ids.extend(ids.flatten(0,1).tolist())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    loss_train = sum(loss_train)/len(loss_train)\n",
    "\n",
    "    initmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_test = []\n",
    "        for seq, coords, ids, _ in test_loader:\n",
    "            pred_test = initmodel(seq)\n",
    "            true_test = pairwise_distance_matrix(coords)\n",
    "            mask = (seq!=0).unsqueeze(1).expand_as(pred_test)\n",
    "            pred_test = pred_test[mask]\n",
    "            true_test = true_test[mask]\n",
    "            loss = criterion(pred, true)\n",
    "            loss_test.append(loss.item())\n",
    "        \n",
    "        loss_test_val = sum(loss_test)/len(loss_test)\n",
    "    \n",
    "    perf.iloc[epoch, :] = [epoch+1, loss_train, loss_test]\n",
    "    print(f\"Epoch {epoch+1}: Loss train {round(loss_train, 2)}, Loss Test {round(loss_test_val, 2)}\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time: {end-start}s\")\n",
    "# TO DO: figure out TM Score (expects 2D dataframe of values), add padding masks, refine whole model, and train on full data, figure out how to import from src/func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf.to_csv(\"../outputs/DistPred/distpred_perf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 35, 35])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_i.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order corresponds between sequences and coordinates\n"
     ]
    }
   ],
   "source": [
    "# Get validation set\n",
    "\n",
    "validation_seq = pd.read_csv(\"../data/validation_sequences.csv\")\n",
    "validation_lbl = pd.read_csv(\"../data/validation_labels.csv\")\n",
    "validation_lbl[\"ID_num\"] = [n+1 for n in range(len(validation_lbl))] # map ID to numeric ID to store in tensor\n",
    "id_mapping_val = {idx+1: og_id for idx, og_id in enumerate(validation_lbl['ID'])} # create mapping to re-map back to original ID\n",
    "id_mapping_val[0] = \"padded_row\"\n",
    "val_set = Rnadataset(validation_seq, validation_lbl)\n",
    "val_loader = DataLoader(val_set,batch_size=32,shuffle=False,num_workers=4,pin_memory=False,collate_fn=collate)\n",
    "\n",
    "\n",
    "# Make predictions on validation set\n",
    "initmodel.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for seq, coords, ids, lengths in val_loader:\n",
    "        pred = initmodel(seq)         \n",
    "        for b in range(pred.size(0)):\n",
    "            single = pred[b]          \n",
    "            mask   = seq[b] != 0\n",
    "            coords = distances_to_coords(single)\n",
    "            coords = coords[mask]\n",
    "            all_preds.append(coords)\n",
    "\n",
    "        \n",
    "\n",
    "stacked = torch.cat(all_preds, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2515, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>resname</th>\n",
       "      <th>resid</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>z_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>z_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>z_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>y_5</th>\n",
       "      <th>z_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R1107_1</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.300216</td>\n",
       "      <td>-10.447870</td>\n",
       "      <td>-0.029812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R1107_2</td>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.032090</td>\n",
       "      <td>0.020420</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R1107_3</td>\n",
       "      <td>G</td>\n",
       "      <td>3</td>\n",
       "      <td>0.029442</td>\n",
       "      <td>-0.015258</td>\n",
       "      <td>-0.035180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R1107_4</td>\n",
       "      <td>G</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018721</td>\n",
       "      <td>-0.016780</td>\n",
       "      <td>-0.196487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R1107_5</td>\n",
       "      <td>G</td>\n",
       "      <td>5</td>\n",
       "      <td>0.017855</td>\n",
       "      <td>-0.018463</td>\n",
       "      <td>-0.123340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID resname  resid        x_1        y_1       z_1  x_2  y_2  z_2  x_3  \\\n",
       "0  R1107_1       G      1 -12.300216 -10.447870 -0.029812  0.0  0.0  0.0  0.0   \n",
       "1  R1107_2       G      2  -0.032090   0.020420  0.204400  0.0  0.0  0.0  0.0   \n",
       "2  R1107_3       G      3   0.029442  -0.015258 -0.035180  0.0  0.0  0.0  0.0   \n",
       "3  R1107_4       G      4   0.018721  -0.016780 -0.196487  0.0  0.0  0.0  0.0   \n",
       "4  R1107_5       G      5   0.017855  -0.018463 -0.123340  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   y_3  z_3  x_4  y_4  z_4  x_5  y_5  z_5  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "submission_cols = ['ID', 'resname', 'resid', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2', 'z_2',\n",
    "       'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "submission_df = pd.DataFrame(0.0, index = range(stacked.size()[0]), columns = submission_cols)\n",
    "submission_df[['ID', 'resname', 'resid']] = validation_lbl[['ID', 'resname', 'resid']]\n",
    "\n",
    "submission_df[['x_1', 'y_1', 'z_1']] = stacked.detach().numpy()\n",
    "submission_df.head()\n",
    "\n",
    "#submission_df.to_csv('../outputs/DistPred/submission2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>resname</th>\n",
       "      <th>resid</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>z_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>z_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>z_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>y_5</th>\n",
       "      <th>z_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R1107_1</td>\n",
       "      <td>G</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.646433</td>\n",
       "      <td>0.406043</td>\n",
       "      <td>-0.092685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R1107_2</td>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>0.013011</td>\n",
       "      <td>0.012916</td>\n",
       "      <td>12.022260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R1107_3</td>\n",
       "      <td>G</td>\n",
       "      <td>3</td>\n",
       "      <td>0.029339</td>\n",
       "      <td>0.017286</td>\n",
       "      <td>-0.329253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R1107_4</td>\n",
       "      <td>G</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032582</td>\n",
       "      <td>0.015530</td>\n",
       "      <td>-0.470709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R1107_5</td>\n",
       "      <td>G</td>\n",
       "      <td>5</td>\n",
       "      <td>0.032398</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>-0.399121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID resname  resid        x_1       y_1        z_1  x_2  y_2  z_2  x_3  \\\n",
       "0  R1107_1       G      1 -13.646433  0.406043  -0.092685  0.0  0.0  0.0  0.0   \n",
       "1  R1107_2       G      2   0.013011  0.012916  12.022260  0.0  0.0  0.0  0.0   \n",
       "2  R1107_3       G      3   0.029339  0.017286  -0.329253  0.0  0.0  0.0  0.0   \n",
       "3  R1107_4       G      4   0.032582  0.015530  -0.470709  0.0  0.0  0.0  0.0   \n",
       "4  R1107_5       G      5   0.032398  0.018278  -0.399121  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   y_3  z_3  x_4  y_4  z_4  x_5  y_5  z_5  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BreakHis",
   "language": "python",
   "name": "breakhis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
